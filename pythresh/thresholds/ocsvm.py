import numpy as np
import scipy.stats as stats
from sklearn.linear_model import RidgeCV
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import OneClassSVM
from sklearn.utils import check_array
from .base import BaseThresholder
from .thresh_utility import normalize, cut, gen_kde

class OCSVM(BaseThresholder):
    """OCSVM class for One-Class Support Vector Machine thresholder.

       Use a one-class svm to evaluate a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are determined by the one-class svm using a polynomial kernel
       with the polynomial degree either set or determined by regression
       internally. See :cite:`barbado2022ocsvm` for details.
       
       Paramaters
       ----------

       degree : int, optional (default='auto')
           Polynomial degree to use for the one-class svm.
           Default 'auto' finds the optimal degree with linear regression

       gamma : float, optional (default='auto')
           Kernel coefficient for polynomial fit for the one-class svm.
           Default 'auto' uses 1 / n_features

       criterion : {'aic', 'bic'}, optional (default='bic')
           regression performance metric. AIC is the Akaike Information Criterion,
           and BIC is the Bayesian Information Criterion. This only applies
           when degree is set to 'auto'

       nu : float, optional (default='auto')
           An upper bound on the fraction of training errors and a lower bound
           of the fraction of support vectors. Default 'auto' sets nu as the ratio
           between the any point that is less than or equal to the median plus
           the absolute difference between the mean and geometric mean over the
           the number of points in the entire dataset 

       tol : float, optional (default=1e-3)
           The stopping criterion for the one-class svm

       Attributes
       ----------

       thres_ : threshold value that seperates inliers from outliers

    """

    def __init__(self, degree='auto', gamma='auto', criterion='bic', nu='auto', tol=1e-3):

        self.degree = degree
        self.gamma = gamma
        self.crit = criterion
        self.nu = nu
        self.tol = tol
        

    def eval(self, decision):
        """Outlier/inlier evaluation process for decision scores.

        Parameters
        ----------
        decision : np.array or list of shape (n_samples)
                   which are the decision scores from a
                   outlier detection.

        Returns
        -------
        outlier_labels : numpy array of shape (n_samples,)
            For each observation, tells whether or not
            it should be considered as an outlier according to the
            fitted model. 0 stands for inliers and 1 for outliers.
        """


        decision = check_array(decision, ensure_2d=False)
        decision = normalize(decision)
        
        # Get auto nu calculation
        if self.nu=='auto':

            np.seterr(divide='ignore')
            gmean = stats.gmean(decision)
            mean = np.mean(decision)
            med = np.median(decision)
            
            self.nu = len(decision[decision<=med+abs(mean-gmean)])/len(decision)

        # Get auto degree calculation
        if self.degree=='auto':

            self.degree = self._auto_crit(decision)

        decision = decision.reshape(-1,1)

        # Create a one-class svm
        clf = OneClassSVM(gamma=self.gamma, kernel='poly',
                          degree=self.degree, nu=self.nu,
                          tol=self.tol).fit(decision)

        # Predict inliers and outliers
        res = clf.predict(decision)

        res[res==-1] = 0
        
        # Remove outliers from the left tail (precaution step)
        decision = np.squeeze(decision)
        mask = np.where(decision<=np.mean(decision))
        res[mask] = 0
        
        self.thresh_ = None

        
        return res
    
    def _auto_crit(self, decision):
        '''Decide polynomial degree using criterion'''
        
        # Generate kde
        kde, dat_range = gen_kde(decision,0,1,len(decision))

        # Set polynomial degrees to test
        polys = [2,3,4,5,6,7,8,9,10]
        n = len(decision)

        decision = decision.reshape(-1,1)
        kde = kde.reshape(-1,1)

        scores = []
        
        for poly in polys:

            # Calculate the polynomial features for the kde
            poly_features = PolynomialFeatures(degree=poly, include_bias=True)
            poly_fit = poly_features.fit_transform(kde)

            # Use regression to fit the polynomial 
            poly_reg = RidgeCV(alphas=np.logspace(-1,2,100))
            poly_reg.fit(poly_fit, dat_range)
            poly_pred = poly_reg.predict(poly_fit)

            # Get the mse and apply the regression performance metric
            mse = mean_squared_error(dat_range, poly_pred)

            if self.crit=='aic':
                scores.append(n*np.log(mse) + 2*(poly+1))
            else:
                scores.append(n*np.log(mse) + (poly+1)*np.log(n))

        # Set degree from smallest metric score
        deg = polys[np.argmin(scores)]
        
        return deg
