{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49955324-442b-434d-8b2b-51499d87ab12",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Method Ranking & Optimal Selection\n",
    "\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "The last notebook showed us how to apply confidences around our threshold methods. But, a question looms over unsupervised tasks, \"what is the best method for my data?\" The utility `RANK` in `PyThresh` attempts to assist in this by ranking all your selected options to tell which is the best performing. How does it do this? Well, for a more in-depth look at what is being done visit [Ranking](https://pythresh.readthedocs.io/en/latest/ranking.html)\n",
    "\n",
    "\n",
    "# Let's get started!\n",
    "\n",
    "To begin, we need to install pythresh and xgboost to work with the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a381da-8029-4a53-8c82-9f6bbd981282",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pythresh xgboost>=2.0.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d974334-af2d-495d-9413-38cb6d0b7faa",
   "metadata": {},
   "source": [
    "We can now import a dataset to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b984668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "from pyod.utils.utility import standardizer\n",
    "\n",
    "file = os.path.join('data', 'cardio.mat')\n",
    "mat = loadmat(file)\n",
    "\n",
    "X = mat['X'].astype(float)\n",
    "y = mat['y'].ravel().astype(int)\n",
    "\n",
    "X = standardizer(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1d22b4-0e5e-4233-a25f-678cdba0b6e8",
   "metadata": {},
   "source": [
    "To rank we must select all the outlier detection methods and thresholders that we want to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84ad491c-87a0-40c4-925e-31578870e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.pca import PCA\n",
    "\n",
    "from pythresh.thresholds.karch import KARCH\n",
    "from pythresh.thresholds.iqr import IQR\n",
    "from pythresh.thresholds.clf import CLF\n",
    "from pythresh.utils.rank import RANK\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "clfs = [KNN(), IForest(random_state=1234), PCA()]\n",
    "thres = [KARCH(), IQR(), CLF()]\n",
    "\n",
    "# Get rankings\n",
    "ranker = RANK(clfs, thres)\n",
    "rankings = ranker.eval(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d3cea0a-e8d6-4048-8ab2-0d5e0219c837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted combo performance from best to worst are [('PCA', 'KARCH'), ('PCA', 'IQR'), ('KNN', 'IQR'), ('IForest', 'IQR'), ('KNN', 'KARCH'), ('PCA', 'CLF'), ('IForest', 'KARCH'), ('IForest', 'CLF'), ('KNN', 'CLF')]\n"
     ]
    }
   ],
   "source": [
    "print(f'The predicted combo performance from best to worst are {rankings}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e6b6c9-3bf6-4154-81a5-78f49f01e138",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "So we got a list of tuples showing the predicted best to worst combos. But let's validate to see how these combos actually perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45841afd-5ce4-463d-abb1-781b572b2d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The f1 and mcc score of comb ('PCA', 'KARCH') are 0.65 and 0.61 respectively\n",
      "\n",
      "The f1 and mcc score of comb ('PCA', 'IQR') are 0.44 and 0.42 respectively\n",
      "\n",
      "The f1 and mcc score of comb ('KNN', 'IQR') are 0.33 and 0.32 respectively\n",
      "\n",
      "The f1 and mcc score of comb ('IForest', 'IQR') are 0.38 and 0.38 respectively\n",
      "\n",
      "The f1 and mcc score of comb ('KNN', 'KARCH') are 0.35 and 0.27 respectively\n",
      "\n",
      "The f1 and mcc score of comb ('PCA', 'CLF') are 0.34 and 0.35 respectively\n",
      "\n",
      "The f1 and mcc score of comb ('IForest', 'KARCH') are 0.55 and 0.51 respectively\n",
      "\n",
      "The f1 and mcc score of comb ('IForest', 'CLF') are 0.44 and 0.41 respectively\n",
      "\n",
      "The f1 and mcc score of comb ('KNN', 'CLF') are 0.25 and 0.26 respectively\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "\n",
    "clfs_dict = {'KNN': KNN(), 'IForest': IForest(random_state=1234), 'PCA': PCA()}\n",
    "thres_dict = {'KARCH': KARCH(), 'IQR': IQR(), 'CLF': CLF()}\n",
    "\n",
    "for comb in rankings:\n",
    "    \n",
    "    clf = clfs_dict[comb[0]]\n",
    "    clf.fit(X)\n",
    "    \n",
    "    scores = clf.decision_scores_ \n",
    "    \n",
    "    thresh = thres_dict[comb[1]]\n",
    "    thresh.fit(scores)\n",
    "    \n",
    "    fit_labels = thresh.labels_\n",
    "    \n",
    "    # How did the unsupervised task perform, lets check the stats\n",
    "    metric1 = round(f1_score(y, fit_labels), 2)\n",
    "    metric2 = round(matthews_corrcoef(y, fit_labels), 2)\n",
    "    \n",
    "    print(f'\\nThe f1 and mcc score of comb {comb} are {metric1} and {metric2} respectively')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0850f486-cd1b-4cc7-9a22-3de990eedfa4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Well the best model was the first ranked, that's great! But note that this is still experimental and may not work well for all datasets and combos. However, exciting news as this is an active area of research for `PyThresh` and further upgrades are planned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d69fda6-713f-4526-a1b0-1271a8458cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a54084e6b208ee8d1ce3989ffc20924477a5f55f5a43e22e699a6741623861e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
